# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/7/19 9:32 AM
 @desc: 本文在于引入SVM，介绍SVM的相关知识点，目的是在心中对SVM有一个大体的认识，本节不涉及代码的编写
"""

"""
 引子：
     SVM对于我而言，一直是一个黑匣子般的存在，为什么呢？函数间隔与几何间隔、间隔最大化的最优化问题、原问题与拉格朗日对偶问题、序列最小最优化
 算法以及核函数，以上都是在解释SVM算法中，都会出现的概念，好像它在对我说，“哦，这个你搞得懂，那接下来的这个你能搞得懂吗? 跪下吧，人类!”，
 抱着“你虽虐我千百遍，我仍待你如初恋”的态度，耐着性子，我尽力的将SVM算法挖掘到我所能到达的深度。
"""

"""
 1.SVM基本概念
    (1)一句话介绍SVM
         支持向量机，英文名Support Vector Machine，一般简称为SVM，通俗来讲，它是一个二分类模型，其基本模型定义为特征空间上的间隔最大的
      线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
 
    (2)什么是线性分类器
         在上一章中，我讲解了logistic regression的算法原理，LR就是一个线性分类器，它是通过Logistic函数(或者sigmoid函数)，将自变量映射
      到(0,1)区间，以计算样本属于某一个类别的概率，最终得到的分类超平面为：w_0*1 + w_1*x_1 + w_2*x_2 = 0，在n维特征空间，分类超平面即为
      w.T * x + b = 0，在SVM中，分类决策函数为 y = sign(w.T * x + b), 其中sign(x)为符号函数。
      
    (3)函数间隔与几何间隔
       一般来说，一个点距离分类超平面的距离，可以表示分类预测的确信程度。假设某个点已正确分类，它离超平面越远，就认为它分类正确的确信度越高。
       在超平面 w.T * x + b = 0确定的情况下，|w.T * x + b|能够相对表示点x到超平面的距离，而(w.T * x + b) 与 y的符号是否一致，可以表示
    分类是否准确，因此为了表示分类的正确性以及确信度，引入了函数间隔的概念：
                                functional_margin = y(w.T*x+b)
       我们知道超平面的表达式为  w.T * x + b = 0，成比例的扩大w,b，超平面位置也不会改变，如2*w.T * x + 2*b = 0，但是函数间隔却变为了
    原来的两倍，因此，为了固定间隔的值，又引入了几何间隔的概念：
                                geometric_margin = y(w.T*x+b) / ||w|| , 其中||w||为L2范数
       可以得出函数间隔与几何间隔的关系为：
                                geometric_margin = functional_margin / ||w||
                                 
    (4)间隔最大化
       支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分类超平面。这个最大间隔分类超平面的问题，可以转换为下面的约束
    最优化问题：
                                max geometric_margin
                                s.t. yi(w.T*xi+b) / ||w|| >= geometric_margin ,其中i=1,2,3,...,N
                                
       用函数间隔代替几何间隔，约束最优化问题转化为：
                                max functional_margin / ||w||
                                s.t. yi(w.T*xi+b) >= functional_margin ,其中i=1,2,3,...,N
                                
    **而我们在前面知道，functional_margin与w,b是成比例变化的，它们所对应的分类超平面，仍然是同一个超平面，知道这一点之后，为了简化计算，
       
       我们不妨将上面的约束最优化问题中的functional_margin值定为1，w,b的取值自然也会成比例的变化，但是最终的分类超平面不会变，由此得到：
                                max 1 / ||w||
                                s.t. yi(w.T*xi+b) >= 1 ,其中i=1,2,3,...,N
                                
       而 max 1 / ||w|| 与 min 1/2 * ||w||^2 二者是等价的，进一步转化上述的约束最优化问题为：
                                min 1/2 * ||w||^2
                                s.t. 1 - yi(w.T*xi+b) <= 0 ,其中i=1,2,3,...,N
                         
       上面的这个表达式，数学基础比较好的人，一眼就能看出，这是一个凸二次规划问题，凸优化问题是指：
                                min f(w)
                                s.t. gi(w)<=0 , i = 1,2,3,...,k,共有k个不等式约束条件
                                     hj(w)=0, j = 1,2,3,...,l , 共有l个等式约束条件
                                     
       由此，我们终于完成了SVM算法的第一步，即定义算法的最优化问题，下面来讲解如何求解这个最优化问题。
                                     
    (5)拉格朗日对偶问题【重中之重】
       当我第一次看到这个名词的时候，第一反应是WTF，看到函数式时，我又立马关上书本，打出手机，播放歌曲，平复心情。。。
    废话还是不多说，其实耐心去看的话，实际上没有想象中的复杂，只是看起来繁琐而已。
          
       “在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过解对偶问题而得到原始问题的解。”    -----《统计学习方法》
        看到这句话，我们心里肯定会想，什么形式的原始问题可以转换为对偶问题？什么是对偶问题？为什么解对偶问题可以得到原始问题的解？ 不着急，
    下面一一来做解释。
    
       (a)原始问题
          假设f(x),ci(x),hj(x)是定义在n维空间上的连续可微函数，考虑约束最优化问题：
                                min f(x) 
                                s.t. ci(x)<=0 , i = 1,2,3,...,k
                                     hj(x)=0 , j = 1,2,3,...,l   
          称此类约束问题为原始问题。
          乍一看，有没有觉得眼熟，没错，我们前面千辛万苦得到的SVM的约束最优化问题，就属于原始问题。
                                min 1/2 * ||w||^2
                                s.t. 1 - yi(w.T*xi+b) <= 0 ,其中i=1,2,3,...,N
                                     
       (b)极小极大问题
          首先引进广义拉格朗日函数，由于约束条件有k+l个，所以有：
          L(x,α,β) = f(x) + α0*c0(x)+α1*c1(x)+...+αk*ck(x) + β0*h0(x)+β0*h0(x)+...+βj*hj(x) ,不好写累加和符号，所以展开式表示
          
          于是，对x有限制条件的f(x)最优化问题，转变为了对x,α,β无限制条件的L(x,α,β)求极值问题，其中，αi,βj是拉格朗日乘子，αi>=0，
       再定义一个函数：
                                θp(x) = max L(x,α,β)  ,  计算在 α,β,αi>=0 条件下的最大值
          不妨来研究一下这个函数:
                                θp(x) = max (f(x) + α0*c0(x)+α1*c1(x)+...+αk*ck(x) + β0*h0(x)+β1*h1(x)+...+βj*hj(x))
                                
          假设某个x，不满足原始问题的约束条件，即有ci(x)>0或者hj(x)!=0，那么上式的结果为+∞，原因如下：
              1)假设c1(x)>0,而α1趋于+∞，其他各项αi,βj都取0时，最终结果为+∞;                         
              2)假设hj(x)!=0,而存在一个βj*hj(x)趋于+∞，其他各项αi,βj都取0时，最终结果还是为+∞;
          相反地，假设x满足约束条件，即ci(x)<=0且hj(x)=0时，而我们定义的αi>0，即每一项αi*ci(x)<=0，那么：    
              L(x,α,β)只能在所有的αk*ck(x)=0和βj*hj(x)同时取0时，才能得到最大值，这时候的最大值即为f(x).  【到这里一定要看懂】
          所以我们就得出来：
                                θp(x) = +∞   ，当x不满足约束条件时   
                                θp(x) = f(x) ，当x满足约束条件时
          根据这个式子，可以得出：
              min θp(x) = min max L(x,α,β) = min f(x)    其中,在min max L(x,α,β)中,min是对x求极小,max是对α,β求极大
          
          我们因此获得一个重要结论：即在x满足限制条件的前提下，min f(x) 等价于 min max L(x,α,β) ，这是一个极小极大问题
       
       (c)对偶问题
          将广义拉格朗日函数的极大极小问题表示为约束最优化问题：
                                max min L(x,α,β)     其中,min是对x求极小，max是对α,β求极大 
                                s.t. αi>=0,i=1,2,3,...,k
          称为原始问题的对偶问题
       
       (d)原始问题与对偶问题的关系
          若原始问题和对偶问题都有最优值，分别为d*,p*，则:
                    d* = max min L(x,α,β) <= min max L(x,α,β) = p*
          
          推论一：
             设 x*和(α*,β*)分别是原始问题和对偶问题的可行解，并且最优值d*=p*，则x*和α*,β*分别是原始问题和对偶问题的最优解。在某些条件下，
          原始问题和对偶问题的最优值相等，即d*=p*，这个时候，就可以用解对偶问题替代解原始问题。
          
          推论二：
             考虑原始问题和对偶问题，假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且假设不等式约束ci(x)是严格可行的，即存在x，对所有i
          有ci(x)<0，则存在x∗和α∗,β∗，使x∗是原始问题的解，α∗,β∗是对偶问题的解，并且有p∗=d∗=L(x∗,α∗,β∗).
          
          推论三：
             考虑原始问题和对偶问题，假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且假设不等式约束ci(x)是严格可行的，则x∗和α∗,β∗分别
          是原始问题、对偶问题的解的充要条件是 x∗和α∗,β∗ 满足下面的KKT条件：
                                ∇x L(x∗,α∗,β∗) = 0
                                ∇α L(x∗,α∗,β∗) = 0
                                ∇β L(x∗,α∗,β∗) = 0                 (1)
                                (αi*)ci(x∗) = 0 , i = 1,2,3,...,k    (2)
                                ci(x∗) <= 0   , i = 1,2,3,...,k    (3)
                                (αi*) >= 0      , i = 1,2,3,...,k    (4)
                                hj(x∗) = 0    , j = 1,2,3,...,k    (5)
             (注：这里面所有的*号都不是乘号，由于下标看起来不够规范，本目录下附了KKT.png文件，看起来更直观一些)
             
             上面的KKT条件中，最重要的是(1)和(2)，(3)、(4)、(5) 都是将原始问题表示成拉格朗日函数时的条件.
             
    ##### 三个推论就是想告诉我们一句话：
             在满足KKT的条件下，原始问题可以转换为对偶问题进行求解，此时二者的最优值相等.
        
          这一部分的内容，主要是参考了《统计学习方法》P225 附录C 拉格朗日对偶性 
       
       (e)对偶问题的解法
          我们已经得出结论，原始问题的对偶问题，是一个极大极小问题，即 max min L(x,α,β) ,为了得到对偶问题的解,
          1)求出L(x,α,β)对x的极小值     【 做法：对x求偏导数,并令其等于0,解出x之后，把x的表达式再代入L(x,α,β)】
          2)代入x值之后，得到一个仅由α,β组成的目标优化函数，通过smo算法进行高效求解
       这一部分的内容，详细请看《统计学习方法》P103 7.1.4 学习的对偶算法
       
    (6) SVM对偶问题的优化目标函数【本文最精华的总结了】
        在第(5)小节中，我花了相当大的篇幅，解释了一般形式的原始问题转对偶问题的求解步骤，现在专门针对SVM算法，对每一个步骤进行说明.
        
        (a)SVM的原始问题
                                min 1/2 * ||w||^2
                                s.t. 1 - yi(w.T*xi+b) <= 0 ,其中i=1,2,3,...,N
                                
        (b)拉格朗日函数转换
                                L(w,b,α) = 1/2 * ||w||^2 + sum αi*(1 - yi(w.T*xi+b))
                    
        (c)现阶段的优化目标函数
                                min max L(w,b,α)   , 其中max是对α求极小，min是对w,b求极大
        
        (d)满足KKT条件下，等价的拉格朗日对偶问题
                                max min L(w,b,α)   , 其中min是对w,b求极小，max是对α求极大
                                
        (e)分别对w,b求偏导，令其值等于0，得到关于w,b的式子，代回L函数，消除w,b，得到仅关于α的优化目标函数
                                max sum(αi) - 1/2 * sum(αi*αj*yi*yj*(xi∙xj))   , 其中i=1,2,3,...,N
                                s.t. sum αi*yi = 0   , L函数对b求偏导，令其值为0，所得
                                     αi >= 0         , 拉格朗日乘子的要求
        (f)max转为min的形式
                                min 1/2 * sum(αi*αj*yi*yj*(xi∙xj)) - sum(αi)   , 其中i=1,2,3,...,N
                                s.t. sum αi*yi = 0
                                     αi >= 0
        
        (g)用smo算法对上式进行求解，得到每一个αi的值，反过来求出w,b，从而得到SVM的分类决策函数(SMO算法之后再具体讲解)
       
        
       (补充：《统计学习方法》P107 例7.2是一个具体的案例题，做完之后能加深算法的理解)
       
 
 2.线性支持向量机的软间隔最大化模型
 
    前面我们所学的内容是，从SVM的分类决策函数，一步步推导出不再包含w,b参数，而是仅由拉格朗日乘子表示的凸二次优化目标函数，通过SMO算法来高效
 求解二次优化问题，最终再根据拉格朗日乘子计算出w和b,得到我们最终的分类决策函数。在这整个过程中，我们一切的假设都是原始数据集是线性可分的，然而
 真实的数据集往往比较复杂，如果原始数据集中混入了一些异常点，导致原始数据集不再线性可分，该怎么办呢？
 
    情况一：本来数据是线性可分的，但是混入了一两个点在数据集里面，导致无法用线性SVM进行分割，见outlier_1.png;
    情况二：数据是线性可分的，但是由于一两个点，导致最大化的间隔距离比较小，模型的泛化能力弱，见outlier_2.png;
    
    为了处理这些异常点的问题，SVM引入了软间隔最大化的方法。
    回顾一下硬间隔最大化的条件：
                              min  1/2 * ||w||^2
                              s.t. yi(w.T*xi+b) >= 1   , 其中,i=1,2,3,...,N
                              
    软间隔最大化呢，就是给每一个样本(xi,yi)引入了一个松弛变量ξi，它的值大于等于0，使函数间隔加上松弛变量后，大于等于1，即
                              s.t. yi(w.T*xi+b) + ξi >= 1   , 其中,i=1,2,3,...,N        
                              
    我们对于样本到超平面的函数距离的要求降低了，但是这个松弛变量是需要被惩罚的，对于引入每一个松弛变量ξi，我们都要把它作为损失的一部分，得：
                              min  1/2 * ||w||^2 + C * sum(ξi) 
                              s.t. yi(w.T*xi+b) + ξi >= 1
                                   ξi >= 0                其中，i=1,2,3,...,N
    C > 0，为惩罚参数，C越大，对于误分类的惩罚也就越大.
    
    我们不妨再变形一下软间隔最大化的原始问题形式：
                              min  1/2 * ||w||^2 + C * sum(ξi) 
                              s.t. 1 - (yi(w.T*xi+b) + ξi) <= 0
                                   -ξi <= 0                其中，i=1,2,3,...,N
    用拉格朗日函数转换为：
        L(w,b,ξ,α,μ) = 1/2 * ||w||^2 + C*sum(ξi) + sum αi*(1 - (yi(w.T*xi+b) + ξi)) + sum μi*(-ξi) , 其中αi,μi都大于等于0
        
    也就是说，我们现在优化的目标函数为：
                              min max L(w,b,ξ,α,μ)    其中,max是对α,μ求极大,min是对w,b,ξ求极小
    转化为等价的拉格朗日对偶问题为:
                              max min L(w,b,ξ,α,μ)    其中,min是对w,b,ξ求极小,max是对α,μ求极大
    
    于是，我们求解过程如下：
        (a)目标函数L分别对w,b,ξ求偏导数，令其等于0，得到三个仅含有αi,μi的等式;
        (b)代入上面的式子到L函数式中，发现w,b以及ξ都可以消除，最终得到一个只包含αi的一元二次函数
        
    重点来了，我们发现，引入了松弛变量之后，最终的优化目标函数和之前硬间隔最大化的目标函数一致，唯一的区别就是约束条件变了，具体如下:
                              max sum(αi) - 1/2 * sum(αi*αj*yi*yj*(xi∙xj))   , 其中i,j=1,2,3,...,N
                              s.t. sum αi*yi = 0         L函数对b求偏导，令其值为0，所得
                                   C-αi-μi = 0  (1)      L函数对ξ求偏导，令其值为0，所得
                                   αi >= 0      (2)      拉格朗日乘子的要求
                                   μi >= 0      (3)      拉格朗日乘子的要求
    
    将max转成min,再把(1),(2),(3)这三个约束条件合并之后，min中不包含μi，所以约束条件去掉μi，最终得到的形式如下：
                              min 1/2 * sum(αi*αj*yi*yj*(xi∙xj)) - sum(αi)
                              s.t. sum αi*yi = 0
                                   0 <= αi <= C
    
    我们发现，软间隔最大化的线性可分SVM的目标形式，和前面提到的硬间隔最大化的线性可分SVM相比，仅仅是多了一个约束条件 0 <= αi <= C 
    我们依然可以通过SMO算法来求出α*向量，进而再求解出w,b !
            
 3.软间隔最大化时的支持向量##
 
    在硬间隔最大化时，支持向量比较简单，满足yi(w.T*xi+b)-1=0即可，根据KKT的对偶互补条件， (αi*) * (yi(w.T*xi+b)-1) = 0,
        如果 αi* > 0 ,那就说明 yi(w.T*xi+b)-1 = 0 , 点(xi,yi)在支持向量上；
        如果 αi* = 0 ,那就说明 yi(w.T*xi+b)-1 >= 0 ,点(xi,yi)在支持向量上，或者已经正确分类       
    
    
    根据软间隔最大化时KKT的对偶互补条件，(αi*) * (yi(w.T*xi+b) + ξi - 1) = 0
        如果 αi* = 0 , 那么yi(w.T*xi+b)-1>=0,说明样本在支持向量所在的间隔边界上，或者已正确分类，分布在远离间隔边界的地方  
        如果 0 < αi* < C ,那么ξi=0, yi(w.T*xi+b)-1 = 0, 样本点在间隔边界上
        如果 αi* = C , 说明这可能是一个异常的点，需要检查此时的ξi
            (a) 0 <= ξi < 1 时，点被正确分类，但是在超平面和自己类别的间隔之间；
            (b) ξi = 1        ，点在分类超平面上，无法正确分类；
            (c) ξi > 1        ，点在超平面的另一侧，不能被正常分类；     
    
  ##现在来解释一下上面对于样本点位置的判断：
  
        首先要明确软间隔最大化的优化目标函数中,KKT的三个条件:
            条件一： (αi*) * (yi(w.T*xi+b)+ξi-1) = 0
            条件二： (μi*) * ξi = 0
            条件三： C - αi - μi = 0
            
        由此可知：
            当 αi* = 0时，KKT条件一显然成立，根据条件三得 μi* = C ,而为了使KKT条件二要成立，则松弛变量ξi=0，三个条件都成立，说明样本点
                正确分类，正确分类就说明 yi(w.T*xi+b)+0-1>=0 , 也就是说样本点要么在间隔边界上，要么在远离间隔边界的地方;
                
            当 0 < αi* < C 时，由条件三可知 0 < μi* < C ，而条件二要成立，则 ξi = 0，此时为了满足条件一，可知yi(w.T*xi+b)+ξi-1 = 0
                必须成立，前面说了ξi = 0，则 yi(w.T*xi+b) - 1 = 0 必须成立，说明样本点必在间隔边界上;
                
            当 αi* = C 时，由条件三知，μi* = 0，进而得出条件二恒成立，ξi的取值不固定，但是它的基本约束条件为ξi>=0，于是分情况讨论:
                (a) 0 <= ξi < 1 时
                    为了使条件一成立，则yi(w.T*xi+b)+ξi-1 = 0，即 0 < yi(w.T*xi+b) <= 1，说明样本点在间隔边界和分类超平面之间
                (b) ξi = 1 时
                    为了使条件一成立，则yi(w.T*xi+b)+ξi-1 = 0，即 yi(w.T*xi+b) = 0 ，说明样本点在分类超平面上
                (c) ξi > 1 时
                    为了使条件一成立，则yi(w.T*xi+b)+ξi-1 = 0，即 yi(w.T*xi+b) < 0 ，说明样本点在分类超平面的另一侧，为误分类点
                    
 4.线性不可分支持向量机与核函数
    (1)回顾多项式回归
       比如一个只有两个特征的p次方多项式回归的模型：
             h_θ(x1,x2) = θ0 + θ1 * x1 + θ2 * x2 + θ3 * x1^2 + θ4 * x2^2 + θ5 * x1 * x2
   
       如果令 x0=1,x1=x1,x2=x2,x3=x1^2,x4=x2^2,x5=x1*x2,这样我们就得到了下式：
             h_θ(x1,x2) = θ0 + θ1 * x1 + θ2 * x2 + θ3 * x3 + θ4 * x4 + θ5 * x5
            
       可以发现，我们又重新回到了线性回归，这是一个五元线性回归，可以用线性回归的方法来完成算法，对于每一个二元样本特征(x1,x2)，我们得到了
    一个五元样本特征(x1,x2,x1^2,x2^2,x1x2)，通过这个改进的五元样本特征，我们重新把不是线性回归的函数变回线性回归.
        
       就是说，对于二维不是线性的数据，我们将其映射到了五维之后，就变成线性的数据了。这给了我们启发，对于SVM线性不可分的低纬度数据，我们
    同样可以映射到高维，就能线性可分。
    
    (2)核函数的引入
       先回顾一下线性可分的SVM的优化目标函数(见目录下的 opt_object_function.png)
                            min 1/2 * sum(αi*αj*yi*yj*(xi∙xj)) - sum(αi)  ,其中i,j = 1,2,3,...,N
                            s.t. sum(αi*yi) = 0
                                 0 <= αi <= C
       注意到上式中，低维特征仅仅以内积xi∙xj形式出现，如果我们定义一个从低维特征空间到高维特征空间的映射ϕ，将特征映射到高维度，让数据
    线性可分，我们就可以继续按照上面的目标函数来求解分类决策函数了，即：
                            min 1/2 * sum(αi*αj*yi*yj(ϕ(xi)∙ϕ(xj))) - sum(αi)  ,其中i,j = 1,2,3,...,N
                            s.t. sum(αi*yi) = 0
                                 0 <= αi <= C
       可以看到，和线性可分的SVM的区别就是仅仅将内积 xi∙xj 替换为 ϕ(xi)∙ϕ(xj)
       
       看起来似乎是完美解决了线性不可分SVM的问题了，假如是2维特征的数据，可以映射到5维空间，如果是3维特征的数据，可以映射到19(3+6+10)维空间,
    随着低维特征不断增长，映射的高维特征则是爆炸性的增长，计算量太大，由此，引入核函数的概念!
   
      假设ϕ是一个低维的输入空间χ到高维的希尔伯特空间的映射，那么存在函数K(x,z),对于任意x,z ∈ χ , 都有：
                                    K(x,z) = ϕ(xi)∙ϕ(xj)
                                    
      我们就称K(x,z)为核函数，仔细观察上式可以发现，K(x,z)的计算是在低维特征空间来计算的，它避免了在刚才我们提到了在高维维度空间计算内积的
    恐怖计算量。也就是说，我们可以好好享受在高维特征空间线性可分的好处，而又避免了高维特征空间恐怖的内积计算量.
    
    (3)核函数的介绍
       事实上，核函数的研究非常的早，比SVM出现早得多，对于从低维到高维的映射，核函数不止一个，而我们一般说的核函数都是正定核函数，下面简单列举
    几个常见的核函数.
    
       (a)线性核函数
          linear kernel其实就是线性可分的SVM，表达式为：
                                    K(x,z) = x∙z
          即：线性可分SVM可以和线性不可分的SVM归为一类，区别仅仅在于线性可分SVM用的核函数为 linear kernel
          
       (b)多项式核函数
          poly kernel是线性不可分SVM常用的核函数之一，表达式为：
                                    K(x,z) = (γ*x∙z + r)^d
          其中，γ,r,d都需要自己调参定义
          
       (c)高斯核函数
          gaussian kernel，在SVM中也称为径向基核函数(RBF),它是非线性分类SVM最主流的核函数，libsvm默认的核函数就是它，表达式为：
                                    K(x,z) = np.exp(-γ*||x-z||^2)
          其中，γ大于0，需要自己调参定义
          
       (d)Sigmoid核函数
          Sigmoid kernel也是线性不可分SVM常用的核函数之一，表达式为：
                                    K(x,z) = tanh(γ*x∙z + r)       
          其中，γ,r需要自己调参定义
          
    核函数的所做的事，就是把SVM的优化目标函数中的(xi∙xj)换成了K(xi,xj),在低维进行计算，而将实质上的分类结果表现在高维上，避免了在高维空间
 中的复杂计算，本质上对我们的优化目标函数没有大的影响，最后还是通过SMO算法进行求解，终于说到了SMO算法了，我将专门在smo_algorithm.py里进行
 讲解，先从原理的角度来解释，再用Python实现简单版本的SMO算法.
"""
