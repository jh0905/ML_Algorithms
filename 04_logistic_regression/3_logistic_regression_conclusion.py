# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/6/19 4:15 PM
 @desc: 逻辑回归算法总结
"""

"""
 前面的学习，我们主要了解了如何从线性回归到逻辑回归，逻辑回归的损失函数的由来，损失函数的最优化求解这三个方面
   
 在总结部分，我想补充三点，二元逻辑回归的正则化，二元逻辑回归推广到多分类，二元逻辑回归的优缺点
    
 1.二元逻辑回归的正则化
    逻辑回归也面临着过拟合的问题，所以我们也要考虑正则化，加一个调节因子，常见的有L1正则化和L2正则化
    (1)L1正则化
       相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数λ作为惩罚系数，调节惩罚项的大小，表达式如下：
       J(θ) = -sum( yi*(ln h(xi)) + (1-yi)*ln(1-h(xi)) + λ*||θ||1，其中||θ||1为θ的L1范数,
       逻辑回归的L1正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法
       
    (2)L2正则化
       J(θ) = -sum( yi*(ln h(xi)) + (1-yi)*ln(1-h(xi)) + λ*(||θ||2)^2，其中(||θ||2)^2为θ的L2范数,
       逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似，用梯度下降法
       
 2.二元逻辑回归的推广：多元逻辑回归
    这就涉及到了二分类模型到多分类模型的转变,
        (1)one-vs-rest(one-vs-all)，简称OvR(OvA)，即总是认为某种类型为正例，其余为负例，
           假设有k个类别，那么就会建立k个二元分类器，每个分类器针对其中一个类别和剩余类别进行分类，进行预测时，利用这k个二元分类器
        进行分类，得到数据属于当前类的概率，选择其中概率最大的一个类别作为最终的预测结果。
        
        (2)one-vs-one，简称OvO，每次从k个类型的样本中，单独选出两种类型的样本出来，建立分类器，
           假设有n个类别，则会针对两两类别建立二元分类器，得到k=n*(n-1)/2个分类器。对新数据进行分类时，依次使用这k个二元分类器进行分类，
        每次分类相当于一次投票，分类结果是哪个就相当于对哪个类投了一票。在使用全部k个分类器进行分类后，相当于进行了k次投票，选择得票最多
        的那个类作为最终分类结果。​

    这里只介绍多元逻辑回归的softmax回归的一种特例推导：
        (1)回顾一下，对于二元逻辑回归，预测测试样本x的类别y的式子为：
           P(y=1|x) = h(x)  = sigmoid(x*θ) = np.exp(x*θ) / (1 + np.exp(x*θ))
           P(y=0|x) = 1 - h(x) = 1 - sigmoid(x*θ) = 1 / (1 + np.exp(x*θ))
           那么两式相除，再取对数，得：
           ln(P(y=1|x) / P(y=0|x)) = x*θ
           
        (2)假如有K个类别，每次选取前(k-1)个类别的中一类样本，和第k类样本，构建二元分类器，得：
                ln(P(y=1|x) / P(y=k|x)) = x*θ1
                ln(P(y=2|x) / P(y=k|x)) = x*θ2
                            ......
                ln(P(y=k-1|x) / P(y=k|x)) = x*θ(k-1)
           其中，每个分类器对应一个参数向量θ，上面总共有(k-1)个方程
           加上概率和为1的方程如下：
                P(y=1|x) + P(y=2|x) + P(y=3|x) +...+P(y=k|x) = 1 
           联立这K个方程,可以求出每一个类型值的概率，看起来复杂，其实这个方程组很好解
                (a)前(k-1)个方程，对数符号右移，得到 P(y=i|x) = np.exp(x*θi) * P(y=k|x)
                (b)将上一步中得到的式子，全部代入P(y=1|x) + P(y=2|x) + P(y=3|x) +...+P(y=k|x) = 1 ，得到P(y=k|x)
                (c)已知P(y=k|x)，根据P(y=i|x) = np.exp(x*θi) * P(y=k|x)，求出P(y=i|x)
           整个过程就是如此简单~
           
 3.二元逻辑回归的优缺点
    LR是解决工业规模问题最流行的算法。在工业应用上，如果需要分类的数据拥有很多有意义的特征，每个特征都对最后的分类结果有或多或少的影响，
 那么最简单最有效的办法就是将这些特征线性加权，一起参与到决策过程中。
    
    优点：
        (1)形式简单，模型的可解释性非常好，特征的权重可以看到不同的特征对最后结果的影响
        (2)除了类别，还能得到近似概率预测，这对许多需利用概率辅助决策的任务很有用
        (3)对率函数是任意阶可导的凸函数，有很好的数学性质
        (4)计算代价不高，速度很快，存储资源低
        (5)将所有数据采用sigmoid函数进行了非线性映射，使得远离分类决策面的数据作用减弱(SVM则是直接去掉了远离分类决策面的样本)
        
    缺点：
        (1)容易欠拟合，分类精度不高
        (2)数据特征有缺失或者特征空间很大时表现效果并不好
        (3)很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.
                我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好
        (4)处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 
        (5)逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归
    
"""
