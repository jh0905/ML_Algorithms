# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/25/19 3:05 PM
 @desc: 学习随机森林的实现原理、几种推广形式、训练样本特征重要性的计算以及算法的优缺点
"""

"""
 引子：
    随机森林是集成学习中可以和梯度提升树GBDT分庭抗礼的算法，由于它的各个弱学习器之间没有依赖关系，因此可以很方便的并行训练，在如今大数据大样本
 的时代很有诱惑力.
 
 1.bagging的原理
    
    (1)bootstrap(自助采样，有放回随机采样)
        从训练集中有放回的采集固定个数的样本，但是每次采集完之后，都将样本放回，也就是说之前采集到的数据，在之后也可能被采集到.
        
        对于bagging，一般会随机采集和训练集样本数m一样个数的样本，因为是有放回采样，直接生成[0,m-1]范围内m个随机数，里面肯定会有一些重复值,
    这样得到的采样集和训练集虽然样本的个数相同，但是样本内容不同.
    
        注意此处与GBDT子采样的区别，GBDT子采样是无放回采样，目的是防止过拟合
        
    (2)OOB(out of bag，袋外数据)
        对于一个样本，它在含m个样本的训练集的某次随机采样中，它被采集到的概率为1/m，不被采集到的概率为(1-1/m)，如果m次采样都没采集到，那概率
    则为(1-1/m)^m . 当m趋于无穷时，(1-1/m)^m则趋于1/e，约等于0.368
        也就是说，如果训练样本集足够大，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采集过，对于这些没有参加过训练的数据，我们
    则称为袋外数据，可以作为验证集，来检测模型的泛化能力.

2.bagging算法流程
    
    输入：样本集D = {(x1,y1), (x2,y2), (x3,y3), ... , (xm,ym)}
    
    输出：强分类器f(x)，回归任务取基模型的均值，分类任务取基模型的多数票
    
    (1) 对 t = 1,2,3,...,T:
        
        (a)对训练集进行第t轮随机采样，共采集m次，得到包含m个样本的子采样集Dt
        
        (b)用采样集Dt训练第t个弱学习器G_t(x)
        
    (2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别为最终类别
        如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出
        
 3.RandomForest(随机森林算法，简称RF)
    
    (1)RF是bagging算法的进化版，做了些独有的改进,具体如下：
    
        (a)RF限定了弱学习器模型为CART决策树(分类与回归)，有点类似GBDT限定了CART回归树
        
        (b)普通的CART决策树，我们在n个样本特征中选择一个最优特征来做决策树的左右子树划分；而RF算法中，每次随机筛选n_sub个特征，n_sub<n，
           然后从n_sub个特征中选择最优特征做决策树的分割点，目的是增强模型的泛化能力，实际过程中，需要通过交叉验证找到一个合适的n_sub值.
    
    (2)随机森林算法流程
        
        输入：样本集D = {(x1,y1), (x2,y2), (x3,y3), ... , (xm,ym)}
    
        输出：强分类器f(x)，回归任务取基模型的均值，分类任务取基模型的多数票
    
        (a) 对 t = 1,2,3,...,T:
        
            (a1)对训练集进行第t轮随机采样，共采集m次，得到包含m个样本的子采样集Dt
        
            (a2)用采样集Dt训练第t个弱学习器G_t(x)
        
        (b) 如果是分类算法预测，则T个弱学习器投出最多票数的类别为最终类别
            如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出

 4.随机森林的推广
    
    (1)extra trees (极端随机森林)，与随机森林区别如下：
        
        (a) 极端随机森林每一个基模型的训练集为原始训练集，没有抽样的操作
        
        (b) 极端随机森林在划分特征的选择上，不是基于信息熵或基尼系数筛选出一个最优划分特征，而是随机的选择一个特征进行分割
        
        由于随机的选择划分特征，导致极端随机森林的基模型树的深度往往较RF更大一些，在某些时候，ERF的泛化能力比RF好
        
    (2)totally random trees embedding (简称TRTE)
    
        TRTE是一种非监督学习的数据转化方法，它能将低维的数据映射到高维，从而更好的运用分类回归模型.(前面学过的核函数也是将低维映射到高维)
        
        TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的
    位置也定下来了。
     
        比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策
    树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0, 0,0,1,0,0,  0,0,0,0,1), 有15维的高维特征,映射到高维特征后，可以继续使用监督学习
    的各种分类回归算法了.
    
    *注：这个思想，怎么和GBDT筛选特征的思想，很相似啊，当然主要区别在于TRTE建树过程是非监督的，而GBDT是监督学习.
    
    (3)Isolation Forest (简称IForest)
        
        (a) IForest是一种异常点检测的方法，对于T个决策树的样本集，IForest也会对训练集进行随机采样，但是采样的个数不需要和RF一样，一般来说
        采样数远远小于训练集的个数，因为我们的目的是异常点检测，只需要部分的样本就可以把异常点区分出来了.
    
        (b) 对于每一棵决策树的建立，IForest也是随机选择一个划分特征，然后对划分特征随机选择一个划分阈值
        
        (c) IForest一般会选择一个比较小的最大决策树深度max_depth，因为训练集比较小，模型不需要那么复杂
        
        (d) 对于异常点的判断，则是将测试样本点x拟合T棵决策树，计算在每棵决策树上该样本的叶子节点的深度值h_t(x)，从而可以计算出平均高度h(x)，
        用下面的公式来计算样本点x的异常概率：
                       
                        s(x,m) = 2 ^ (-h(x)/c(m))           ，s(x,m)的取值范围是[0,1],取值越接近于1，则是异常点的概率也越大。
            
        其中，m为样本点的个数，c(m)的表达式为：
                        c(m) = 2*ln(m-1) + ξ - 2*(m-1)/m    ，ξ为欧拉常数0.5772156649
 
 5.利用随机森林做特征选择 【干货哟!】
    根据不同的度量指标，来对数据集中各个特征的重要度进行排序，主要分为基尼指数和袋外数据(OOB)错误率这两种方式.
    
    (1)基于OOB错误率的度量
    
        (a)对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB_i
        
        (b)随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB_j
         
        (c)假设随机森林中有N棵树,那么对于特征X的重要性 = (sum errOOB_j- sum errOOB_i)/N
         
    用这个表达式来作为相应特征的重要性的度量值是因为：
        若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高
        
    (2)基于基尼指数的度量
        
        (a)先回顾一下基尼指数的计算：
            
            (a1)假设数据集中有n个类，样本点属于第i类的概率为pi,pi=第i类的样本数/总样本数，那么该数据集的基尼指数为:
            
                Gini(p) = 1 - sum(pi**2) ,其中i∈[1,n]  特别地，对于2分类任务，直接变形为 2×p*(1-p),计算更加简便!
           
            (a2)如果数据集D根据特征A是否取某一特征值a,被分割成D1,D2两部分，则在特征A的条件下，集合D的基尼指数定义为:
                
                Gini(D,A) = |D1|/|D| * Gini(D1) + |D2|/|D| * Gini(D2), 其中|D|表示数据集D的样本个数
           
            #如果特征A有两个以上的特征值，那么必须计算出Gini(D,A=a1),Gini(D,A=a2),Gini(D,A=a3)...,选择最小的那一个作为基尼指数,参与
             本轮的划分属性评选，最小的基尼指数对应的特征和特征值作为该数据集上的二元切分点!
        
        (b)单棵CART决策树如何计算特征重要性?
            
            先通过一个实例，具体了解一下特征重要性是怎么计算的 (打开本目录下的cart_feature_importance.png)，该图是某棵决策树的结构图
            
            计算特征A3的重要度： A3的基尼指数*样本数 - (A3左孩子的基尼指数*样本数 + A3右孩子的基尼指数*样本数)
            即 importance(A3) = 0.48*15 - (0.4444*9 + 0.0*6) = 3.2004
            
            同理特征A2的重要度： A2的基尼指数*样本数 - (A2左孩子的基尼指数*样本数 + A2右孩子的基尼指数*样本数)
            即 importance(A2) = 0.4444*9 - (0.0*6 + 0.0*3) = 3.9996
            
            图中没有出现的特征A1，A4的重要度即为0.
            
            所以该棵树上所有节点总的加权不纯度减少量为 3.2004+3.9996=7.3  
            对其进行归一化操作可以得到A1、A2、A3、A4的特征重要性为 [ 0. 0.55555556 0.44444444 0. ]
        
        注意： 
            本例是一个比较简单的例子，如果某个特征在单棵决策树中不止一次作为划分节点，那么要计算所有出现该特征的地方，计算一次重要度，方法和
        上面一样，特征A的基尼指数*样本数 - (特征A左孩子的基尼指数*样本数 + 特征A右孩子的基尼指数*样本数)，最终累加起来，作为该特征A的重要度.
        
        (c)随机森林怎么计算特征的重要性?
            
            知道前面计算单棵CART树的特征重要性了，那么决策森林中，也就简单了，把某个特征A在所有树中的重要度求和，即为该特征A整体的重要性，
        然后分别计算出每个特征在随机森林中的重要性，求出重要性之和，再对单个特征进行归一化处理，从而完成随机森林特征重要性的整个计算工作.   
            
 6.随机森林算法小结
    
    作为一个可以高度并行化的算法，RF在大数据时候大有可为.
    
    优点：
        (1)训练可以高度并行化，对于大数据时代的大样本训练速度很有优势（最主要的优点）
        (2)由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型
        (3)在训练后，可以给出各个特征对于输出的重要性
        (4)由于采用了随机采样，训练出的模型的方差小，泛化能力强
        (5)对部分特征缺失不敏感
        
    缺点：
        (1)取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果
        (2)随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟
"""
